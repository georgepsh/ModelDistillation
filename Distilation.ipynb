{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n",
    "from torchtext import data\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/george/Documents/Distillation'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('/Users/george/Documents/Distillation/')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Решаю задачу классификации смс-сообщений на спам и не спам"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обработка текстовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(X):\n",
    "    words = [sentence.split() for sentence in X]\n",
    "    text_field = data.Field()\n",
    "    text_field.build_vocab(words, max_size=10000)\n",
    "    return text_field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(seq, maxlen):\n",
    "    if len(seq) < maxlen:\n",
    "        seq = seq + ['<pad>'] * (maxlen - len(seq))\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_indices(vocab, words):\n",
    "    return [vocab.stoi[w] for w in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dataset(X, y, field):\n",
    "    X_split = [t.split() for t in X]\n",
    "    X_pad = [pad(s, maxlen) for s in X_split]\n",
    "    X_index = [to_indices(field.vocab, s) for s in X_pad]\n",
    "    torch_x = torch.tensor(X_index, dtype=torch.long)\n",
    "    torch_y = torch.tensor(y, dtype=torch.float)\n",
    "    return TensorDataset(torch_x, torch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dataset_distill(X, y, teacher_output, field):\n",
    "    X_split = [t.split() for t in X]\n",
    "    X_pad = [pad(s, maxlen) for s in X_split]\n",
    "    X_index = [to_indices(field.vocab, s) for s in X_pad]\n",
    "    torch_x = torch.tensor(X_index, dtype=torch.long)\n",
    "    torch_y = torch.tensor(y, dtype=torch.float)\n",
    "    return TensorDataset(torch_x, torch_y, teacher_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dataset_for_bert(X, y, tokenizer):\n",
    "    X_split = [t.split() for t in X]\n",
    "    text = [pad(s, maxlen) for s in X_split]\n",
    "    lines = [\" \".join(s) for s in text]\n",
    "    masks = [[int(word != '<pad>') for word in sentence] for sentence in text]\n",
    "    inds = [tokenizer.encode(line.split(), add_special_tokens=False) for line in lines]\n",
    "    inds = torch.tensor(inds)\n",
    "    masks = torch.tensor(masks, dtype=torch.int8)\n",
    "    torch_y = torch.tensor(y, dtype=torch.float)\n",
    "    return TensorDataset(inds, torch_y, masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_preprocess_spam_data(path):\n",
    "    X = []\n",
    "    y = []\n",
    "    maxlen = 0\n",
    "    with open(os.getcwd() + path) as file:\n",
    "        for line in file:\n",
    "            words = line.split()\n",
    "            y.append(0 if words[0] == 'ham' else 1)\n",
    "            X.append(' '.join(words[1:]))\n",
    "            maxlen = max(maxlen, len(words))\n",
    "    return X, y, maxlen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Модель учитель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "в качестве учителя я взял предобученный Берт для классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функция потерь для дистилляции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistillLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.5):\n",
    "        super(DistillLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def forward(self, real_prediction, real_output, teacher_prediction, teacher_output):\n",
    "        bce = nn.CrossEntropyLoss()\n",
    "        mse = nn.MSELoss()\n",
    "        prediction_loss = bce(real_prediction, torch.tensor(real_output, dtype=torch.long))\n",
    "        teacher_loss = mse(teacher_prediction, teacher_output)\n",
    "        return self.alpha * prediction_loss + (1 - self.alpha) * teacher_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, \\\n",
    "                bidirectional, dropout, num_layers):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        \n",
    "        self.rnn = nn.LSTM(\n",
    "                            input_size=embedding_dim, \n",
    "                            hidden_size=hidden_dim, \n",
    "                            num_layers=num_layers,\n",
    "                            bidirectional=bidirectional,\n",
    "                            dropout=dropout\n",
    "                        )\n",
    "        \n",
    "        self.label_prediction = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def init_state(self, batch_size):\n",
    "        return torch.zeros(2 * self.num_layers, batch_size, self.hidden_dim), \\\n",
    "               torch.zeros(2 * self.num_layers, batch_size, self.hidden_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = torch.transpose(x, dim0=1, dim1=0)\n",
    "        x, hidden = self.rnn(x)\n",
    "        hidden, cell = hidden\n",
    "        hidden = self.dropout(torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1))\n",
    "        label_prediction = self.label_prediction(hidden)\n",
    "        return label_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_labels=2):\n",
    "        super(ClassificationHead, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, num_labels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        return self.fc3(x)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TeacherModel(nn.Module):\n",
    "    def __init__(self, teacher, head_hidden_size=128):\n",
    "        super(TeacherModel, self).__init__()\n",
    "        self.teacher = teacher\n",
    "        hidden_size = self.teacher.config.hidden_size\n",
    "        self.classification_head = ClassificationHead(hidden_size, head_hidden_size, 2)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        inds = inp[0]\n",
    "        labels = inp[1]\n",
    "        masks = inp[2]\n",
    "        labels = torch.tensor(labels, dtype=torch.long)\n",
    "        output = self.teacher(inds, attention_mask=masks)[0]\n",
    "        output = output[:, 0, :]\n",
    "        prediction = self.classification_head(output)\n",
    "        loss = self.loss(prediction, labels)\n",
    "        return loss\n",
    "    \n",
    "    def inference(self, inp):\n",
    "        inds = inp[0]\n",
    "        masks = inp[2]\n",
    "        output = self.teacher(inds, attention_mask=masks)[0]\n",
    "        output = output[:, 0, :]\n",
    "        prediction = self.classification_head(output)\n",
    "        return prediction\n",
    "    \n",
    "    def parameters(self):\n",
    "        return self.classification_head.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, bilstm):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.bilstm = bilstm\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        inds = inp[0]\n",
    "        labels = inp[1]\n",
    "        labels = torch.tensor(labels, dtype=torch.long)\n",
    "        prediction = self.bilstm(inds)\n",
    "        loss = self.loss(prediction, labels)\n",
    "        return loss\n",
    "    \n",
    "    def inference(self, inp):\n",
    "        inds = inp[0]\n",
    "        return self.bilstm(inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistillModel(nn.Module):\n",
    "    def __init__(self, student, alpha=0.5):\n",
    "        super(DistillModel, self).__init__()\n",
    "        self.student = student\n",
    "        self.loss = DistillLoss(alpha)\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        inds = inp[0]\n",
    "        labels = inp[1]\n",
    "        teacher_output = inp[2]\n",
    "        labels = torch.tensor(labels, dtype=torch.long)\n",
    "        label_prediction = self.student(inds)\n",
    "        loss = self.loss(label_prediction, labels, label_prediction, teacher_output)\n",
    "        return loss\n",
    "    \n",
    "    def inference(self, inp):\n",
    "        inds = inp[0]\n",
    "        return self.student(inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_teacher_output(teacher, dataset):\n",
    "    dataloader = DataLoader(dataset, 20, shuffle=False)\n",
    "    teacher_output = []\n",
    "    for info in dataloader: # прогоняю батчами, потому что кернел падает, если считать все сразу\n",
    "        result = teacher.inference(info).detach()\n",
    "        teacher_output.append(result)\n",
    "    teacher_output = torch.cat(teacher_output)\n",
    "    return teacher_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataset, epochs=5, batch_size=64):\n",
    "    dataloader = DataLoader(dataset, batch_size, shuffle=True)\n",
    "    epoch_loss = []\n",
    "    optimizer = optim.Adam(model.parameters())    \n",
    "    print('training started...')\n",
    "    model.train()\n",
    "    for e in range(epochs):\n",
    "        losses = 0\n",
    "        count = 0\n",
    "        print(f'epoch {e} loss:', end=' ')\n",
    "        for info in dataloader:\n",
    "            loss = model(info)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses += loss\n",
    "            count += 1\n",
    "        losses /= count\n",
    "        print(losses.item())\n",
    "        epoch_loss.append(losses)\n",
    "    print('training finished...')\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функция для измерения точности модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_accuracy(model, dataset):\n",
    "    dataloader = DataLoader(dataset, 1, shuffle=True)\n",
    "    correct = 0\n",
    "    count = 0\n",
    "    positive = 0\n",
    "    true_positive = 0\n",
    "    for info in dataloader:\n",
    "        prediction = torch.argmax(model.inference(info))\n",
    "        correct += int(prediction.item() == int(info[1]))\n",
    "        count += 1\n",
    "        positive += int(int(prediction.item()) and int(info[1]))\n",
    "        true_positive += int(info[1])\n",
    "    print(positive / true_positive)\n",
    "    return correct / count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Прочитаем данные и поделим на трейн и тест"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, maxlen = read_and_preprocess_spam_data('/data/dataset.txt')\n",
    "field = get_vocab(X)\n",
    "vocab_size = len(field.vocab.stoi.keys())\n",
    "# ham_data = [x for x in zip(X, y) if x[1] == 0]\n",
    "# spam_data = [x for x in zip(X, y) if x[1] == 1]\n",
    "# spam_count = len(spam_data)\n",
    "# ham_data_subset = random.sample(ham_data, spam_count)\n",
    "# sms_data = ham_data_subset + spam_data\n",
    "# X, y = list(zip(*sms_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4180\n",
      "1394\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, test_size=0.25)\n",
    "print(len(X_train))\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим отдельные датасеты для обучения модели без учителя и для обучения берта"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_dataset_train = to_dataset(X_train, y_train, field)\n",
    "teacher_dataset_train = to_dataset_for_bert(X_train, y_train, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_dataset_test = to_dataset(X_test, y_test, field)\n",
    "teacher_dataset_test = to_dataset_for_bert(X_test, y_test, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Обучим Берт"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher = TeacherModel(bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training started...\n",
      "epoch 0 loss: 0.332807332277298\n",
      "epoch 1 loss: 0.2568117678165436\n",
      "epoch 2 loss: 0.2305523306131363\n",
      "epoch 3 loss: 0.2213379293680191\n",
      "epoch 4 loss: 0.2192654311656952\n",
      "epoch 5 loss: 0.21612565219402313\n",
      "epoch 6 loss: 0.21643000841140747\n",
      "epoch 7 loss: 0.2127368301153183\n",
      "training finished...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor(0.3328, grad_fn=<DivBackward0>),\n",
       " tensor(0.2568, grad_fn=<DivBackward0>),\n",
       " tensor(0.2306, grad_fn=<DivBackward0>),\n",
       " tensor(0.2213, grad_fn=<DivBackward0>),\n",
       " tensor(0.2193, grad_fn=<DivBackward0>),\n",
       " tensor(0.2161, grad_fn=<DivBackward0>),\n",
       " tensor(0.2164, grad_fn=<DivBackward0>),\n",
       " tensor(0.2127, grad_fn=<DivBackward0>)]"
      ]
     },
     "execution_count": 574,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(teacher, teacher_dataset_train, epochs=8, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6042780748663101\n",
      "teacher accuracy: 0.926829268292683\n"
     ]
    }
   ],
   "source": [
    "print('teacher accuracy:', measure_accuracy(teacher, teacher_dataset_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Обучим обычный BiLSTM без учителя"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiLSTM(input_dim=vocab_size, \n",
    "               embedding_dim=16,\n",
    "               hidden_dim=16, \n",
    "               output_dim=2,\n",
    "               bidirectional=True,\n",
    "               dropout=0.5,\n",
    "               num_layers=1,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_model = SimpleModel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training started...\n",
      "epoch 0 loss: 0.5213630199432373\n",
      "epoch 1 loss: 0.4039366543292999\n",
      "epoch 2 loss: 0.3245985209941864\n",
      "epoch 3 loss: 0.28657665848731995\n",
      "epoch 4 loss: 0.26641058921813965\n",
      "epoch 5 loss: 0.2391241192817688\n",
      "epoch 6 loss: 0.2133490890264511\n",
      "epoch 7 loss: 0.19731676578521729\n",
      "training finished...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor(0.5214, grad_fn=<DivBackward0>),\n",
       " tensor(0.4039, grad_fn=<DivBackward0>),\n",
       " tensor(0.3246, grad_fn=<DivBackward0>),\n",
       " tensor(0.2866, grad_fn=<DivBackward0>),\n",
       " tensor(0.2664, grad_fn=<DivBackward0>),\n",
       " tensor(0.2391, grad_fn=<DivBackward0>),\n",
       " tensor(0.2133, grad_fn=<DivBackward0>),\n",
       " tensor(0.1973, grad_fn=<DivBackward0>)]"
      ]
     },
     "execution_count": 647,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(simple_model, simple_dataset_train, epochs=8, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6844919786096256\n",
      "simple model accuracy: 0.9225251076040172\n"
     ]
    }
   ],
   "source": [
    "print('simple model accuracy:', measure_accuracy(simple_model, simple_dataset_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Применим дистилляцию"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "посчитаем выходы берта для данного датасета и положим их в отдельный датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 626,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_output_train = get_teacher_output(teacher, teacher_dataset_train)\n",
    "teacher_output_test = get_teacher_output(teacher, teacher_dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [],
   "source": [
    "distill_dataset_train = to_dataset_distill(X_train, y_train, teacher_output_train, field)\n",
    "distill_dataset_test = to_dataset_distill(X_test, y_test, teacher_output_test, field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [],
   "source": [
    "distill_model = BiLSTM(input_dim=vocab_size, \n",
    "               embedding_dim=16,\n",
    "               hidden_dim=16, \n",
    "               output_dim=2,\n",
    "               bidirectional=True,\n",
    "               dropout=0.8,\n",
    "               num_layers=1,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [],
   "source": [
    "student = DistillModel(distill_model, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training started...\n",
      "epoch 0 loss: 1.96905517578125\n",
      "epoch 1 loss: 1.2974913120269775\n",
      "epoch 2 loss: 1.088428258895874\n",
      "epoch 3 loss: 1.043113112449646\n",
      "epoch 4 loss: 1.0074676275253296\n",
      "epoch 5 loss: 0.9557787775993347\n",
      "epoch 6 loss: 0.920454204082489\n",
      "epoch 7 loss: 0.8890780210494995\n",
      "training finished...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor(1.9691, grad_fn=<DivBackward0>),\n",
       " tensor(1.2975, grad_fn=<DivBackward0>),\n",
       " tensor(1.0884, grad_fn=<DivBackward0>),\n",
       " tensor(1.0431, grad_fn=<DivBackward0>),\n",
       " tensor(1.0075, grad_fn=<DivBackward0>),\n",
       " tensor(0.9558, grad_fn=<DivBackward0>),\n",
       " tensor(0.9205, grad_fn=<DivBackward0>),\n",
       " tensor(0.8891, grad_fn=<DivBackward0>)]"
      ]
     },
     "execution_count": 651,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(student, distill_dataset_train, epochs=8, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.026737967914438502\n",
      "student model accuracy: 0.8687230989956959\n"
     ]
    }
   ],
   "source": [
    "print('student model accuracy:', measure_accuracy(student, distill_dataset_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
