{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n",
    "from torchtext import data\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Решаю задачу классификации смс-сообщений на спам и не спам"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обработка текстовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(X):\n",
    "    words = [sentence.split() for sentence in X]\n",
    "    text_field = data.Field()\n",
    "    text_field.build_vocab(words, max_size=10000)\n",
    "    return text_field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(seq, maxlen):\n",
    "    if len(seq) < maxlen:\n",
    "        seq = seq + ['<pad>'] * (maxlen - len(seq))\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_indices(vocab, words):\n",
    "    return [vocab.stoi[w] for w in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dataset(x, y, teacher_output):\n",
    "    torch_x = torch.tensor(x, dtype=torch.long)\n",
    "    torch_y = torch.tensor(y, dtype=torch.float)\n",
    "    if teacher_output is None:\n",
    "        torch_teacher_output = torch.full_like(torch_y, 0)\n",
    "    else:\n",
    "        torch_teacher_output = teacher_output\n",
    "    return TensorDataset(torch_x, torch_y, torch_teacher_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_preprocess_spam_data(path):\n",
    "    X = []\n",
    "    y = []\n",
    "    maxlen = 0\n",
    "    with open(os.getcwd() + path) as file:\n",
    "        for line in file:\n",
    "            words = line.split()\n",
    "            y.append(0 if words[0] == 'ham' else 1)\n",
    "            X.append(' '.join(words[1:]))\n",
    "            maxlen = max(maxlen, len(words))\n",
    "    return X, y, maxlen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Модель учитель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "в качестве учителя я взял предобученный Берт для классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TeacherModel(nn.Module):\n",
    "    def __init__(self, teacher):\n",
    "        super(TeacherModel, self).__init__()\n",
    "        self.teacher = teacher\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.teacher(x)[0], 0 # здесь я возвращаю кортеж с фиктивным вторым элементом,\n",
    "                                    # чтобы можно было дообучить Берт используя тот же код,\n",
    "                                    # который я использую для обучения модели ученика"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Модель ученик"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, \\\n",
    "                bidirectional, dropout, num_layers):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        \n",
    "        self.rnn = nn.LSTM(\n",
    "                            input_size=embedding_dim, \n",
    "                            hidden_size=hidden_dim, \n",
    "                            num_layers=num_layers,\n",
    "                            bidirectional=bidirectional,\n",
    "                            dropout=dropout\n",
    "                        )\n",
    "        \n",
    "        self.label_prediction = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.teacher_prediction = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def init_state(self, batch_size):\n",
    "        return torch.zeros(2 * self.num_layers, batch_size, self.hidden_dim), \\\n",
    "               torch.zeros(2 * self.num_layers, batch_size, self.hidden_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = torch.transpose(x, dim0=1, dim1=0)\n",
    "        x, hidden = self.rnn(x)\n",
    "        hidden, cell = hidden\n",
    "        hidden = self.dropout(torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1))\n",
    "        label_prediction = self.label_prediction(hidden)\n",
    "        teacher_prediction = self.teacher_prediction(hidden)\n",
    "        return label_prediction, teacher_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функция потерь для дистилляции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistilLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.5):\n",
    "        super(DistilLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def forward(self, real_prediction, real_output, teacher_prediction=None, teacher_output=None):\n",
    "        bce = nn.CrossEntropyLoss()\n",
    "        mse = nn.MSELoss()\n",
    "        prediction_loss = bce(real_prediction, torch.tensor(real_output, dtype=torch.long))\n",
    "        if teacher_output is None:\n",
    "            return prediction_loss # если учимся без учителя, то обычная кросс-энтропия\n",
    "        teacher_loss = mse(teacher_prediction, teacher_output)\n",
    "        return self.alpha * prediction_loss + (1 - self.alpha) * teacher_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, field, X, y, maxlen, epochs=5, batch_size=64, teacher=None, alpha=0.5):\n",
    "    X_split = [t.split() for t in X]\n",
    "    X_pad = [pad(s, maxlen) for s in X_split]\n",
    "    X_index = [to_indices(field.vocab, s) for s in X_pad]\n",
    "    teacher_output = None\n",
    "    if teacher: # делаю предподсчет выходов модели учителя, чтобы использовать их во время обучения\n",
    "        print('calculating teacher output...')\n",
    "        lines = [\" \".join(s) for s in X_pad]\n",
    "        inds = [tokenizer.encode(line.split(), add_special_tokens=False) for line in lines]\n",
    "        inds = torch.tensor(inds)\n",
    "        teacher_output = []\n",
    "        for i in range(len(inds) // 20): # прогоняю батчами, потому что кернел падает, если считать все сразу\n",
    "            result = teacher(inds[i * 20: (i + 1) * 20])[0].detach()\n",
    "            teacher_output.append(result)\n",
    "        teacher_output = torch.cat(teacher_output)\n",
    "        print('finished calculating teacher output...')\n",
    "        print()\n",
    "\n",
    "    dataset = to_dataset(X_index, y, teacher_output)\n",
    "    dataloader = DataLoader(dataset, batch_size, shuffle=True)\n",
    "    \n",
    "    epoch_loss = []\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    loss_function = DistilLoss(alpha=alpha)\n",
    "    \n",
    "    print('training started...')\n",
    "    for e in range(epochs):\n",
    "        losses = 0\n",
    "        count = 0\n",
    "        print(f'epoch {e}', end=' ')\n",
    "        for X_batch, y_batch, y_teacher in dataloader:\n",
    "            label_prediction, teacher_prediction = model(X_batch)\n",
    "            teacher_output = None\n",
    "            if teacher:\n",
    "                teacher_output = y_teacher\n",
    "            labels = torch.tensor(y_batch, dtype=torch.long)\n",
    "            loss = loss_function(label_prediction, labels, teacher_prediction, teacher_output)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses += loss\n",
    "            count += 1\n",
    "        losses /= count\n",
    "        print(losses)\n",
    "        epoch_loss.append(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функция для измерения точности модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_accuracy(model, field, X, y, maxlen):\n",
    "    X_split = [t.split() for t in X]\n",
    "    X_pad = [pad(s, maxlen) for s in X_split]\n",
    "    X_index = [to_indices(field.vocab, s) for s in X_pad]\n",
    "    dataset = to_dataset(X_index, y, None)\n",
    "    dataloader = DataLoader(dataset, 1, shuffle=True)\n",
    "    correct = 0\n",
    "    count = 0\n",
    "    for sample, label, _ in dataloader:\n",
    "        prediction = torch.argmax(model(sample)[0])\n",
    "        correct += int(prediction.item() == int(label))\n",
    "        count += 1\n",
    "    return correct / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, maxlen = read_and_preprocess_spam_data('dataset.txt')\n",
    "field = get_vocab(X)\n",
    "vocab_size = len(field.vocab.stoi.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "Обычный BiLSTM обучался очень хорошо на данном датасете, поэтом я решил выбрать всего 1000 примеров  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train lenghts: 1000 1000\n",
      "test lengths: 4574 4574\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, test_size=0.8205)\n",
    "print('train lenghts:', len(X_train), len(y_train))\n",
    "print('test lengths:', len(X_test), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiLSTM(input_dim=vocab_size, \n",
    "               embedding_dim=64,\n",
    "               hidden_dim=32, \n",
    "               output_dim=2,\n",
    "               bidirectional=True,\n",
    "               dropout=0.5,\n",
    "               num_layers=1,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь я решил немного дообучить Берт на данном датасете"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher = TeacherModel(bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training started...\n",
      "epoch 0 tensor(0.4848, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "train(teacher, field, X_train, y_train, maxlen, teacher=None, epochs=1, batch_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Обучение обычной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training started...\n",
      "epoch 0 tensor(0.5167, grad_fn=<DivBackward0>)\n",
      "epoch 1 tensor(0.4005, grad_fn=<DivBackward0>)\n",
      "epoch 2 tensor(0.3491, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "train(model, field, X_train, y_train, maxlen, epochs=3, batch_size=50, teacher=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simple model accuracy: 0.8692610406646262\n"
     ]
    }
   ],
   "source": [
    "print('simple model accuracy:', measure_accuracy(model, field, X_test, y_test, maxlen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Теперь обучим модель с дистилляцией"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "distill_model = BiLSTM(input_dim=vocab_size, \n",
    "               embedding_dim=64,\n",
    "               hidden_dim=32, \n",
    "               output_dim=2,\n",
    "               bidirectional=True,\n",
    "               dropout=0.5,\n",
    "               num_layers=1,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Коэффициент альфу я выбрал равным 0.1, то есть модель обучалась в большей степени на лоссе \"подражания\", чем на кросс-энтропии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating teacher output...\n",
      "finished calculating teacher output...\n",
      "\n",
      "training started...\n",
      "epoch 0 tensor(0.0799, grad_fn=<DivBackward0>)\n",
      "epoch 1 tensor(0.0597, grad_fn=<DivBackward0>)\n",
      "epoch 2 tensor(0.0540, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "train(distill_model, field, X_train, y_train, maxlen, epochs=3, batch_size=50, teacher=teacher, alpha=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно, несмотря на то, что модель в меньшей степени обучалась на кросс-энтропии, получилось добиться даже более высокой точности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilled model accuracy: 0.9007433318758199\n"
     ]
    }
   ],
   "source": [
    "print('distilled model accuracy:', measure_accuracy(distill_model, field, X_test, y_test, maxlen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
